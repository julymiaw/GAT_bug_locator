#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è‡ªé€‚åº”GATè®­ç»ƒè¯„ä¼°è„šæœ¬

ç”¨é€”: åˆ†æè‡ªé€‚åº”è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„å„ç§æ—¥å¿—æ–‡ä»¶ï¼Œæä¾›è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œå¯è§†åŒ–
è¾“å…¥: è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„JSONæ—¥å¿—æ–‡ä»¶
è¾“å‡º: è¯¦ç»†çš„åˆ†æç»“æœã€ç»Ÿè®¡æ•°æ®å’Œå¯è§†åŒ–å›¾è¡¨

ä½œè€…: SRTPå›¢é˜Ÿ
æ—¥æœŸ: 2025å¹´3æœˆ7æ—¥
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
from typing import Dict, List, Any
import argparse
from train_utils import eprint

# Linuxä¸‹è®¾ç½®
mpl.rcParams["font.sans-serif"] = ["SimHei"]  # è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºé»‘ä½“
mpl.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜


class AdaptiveGATEvaluator:
    """è‡ªé€‚åº”GATè®­ç»ƒè¯„ä¼°å™¨ï¼Œç”¨äºåˆ†æè®­ç»ƒæ—¥å¿—å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""

    def __init__(self, log_dir: str):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        å‚æ•°:
            log_dir: åŒ…å«è®­ç»ƒæ—¥å¿—çš„ç›®å½•è·¯å¾„
        """
        self.log_dir = log_dir
        self.output_dir = os.path.join(log_dir, "analysis_results")
        os.makedirs(self.output_dir, exist_ok=True)

        # æ—¥å¿—æ•°æ®
        self.training_time = {}
        self.regression_log = {}
        self.prescoring_log = {}
        self.prediction_log = {}
        self.best_regression = {}
        self.best_prescoring = {}

        # åˆ†æç»“æœ
        self.model_performance_df = None

    def load_logs(self):
        """åŠ è½½æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        print("æ­£åœ¨åŠ è½½æ—¥å¿—æ–‡ä»¶...")

        # è®­ç»ƒæ—¶é—´æ—¥å¿—
        try:
            with open(
                os.path.join(self.log_dir, "Adaptive_training_time.json"), "r"
            ) as f:
                self.training_time = json.load(f)
            print("âœ“ æˆåŠŸåŠ è½½è®­ç»ƒæ—¶é—´æ—¥å¿—")
        except Exception as e:
            print(f"âœ— æ— æ³•åŠ è½½è®­ç»ƒæ—¶é—´æ—¥å¿—: {e}")

        # å›å½’æ¨¡å‹æ—¥å¿—
        try:
            with open(
                os.path.join(self.log_dir, "Adaptive_regression_log.json"), "r"
            ) as f:
                self.regression_log = json.load(f)
            print("âœ“ æˆåŠŸåŠ è½½å›å½’æ¨¡å‹æ—¥å¿—")
        except Exception as e:
            print(f"âœ— æ— æ³•åŠ è½½å›å½’æ¨¡å‹æ—¥å¿—: {e}")

        # é¢„è¯„åˆ†æ—¥å¿—
        try:
            with open(
                os.path.join(self.log_dir, "Adaptive_prescoring_log.json"), "r"
            ) as f:
                self.prescoring_log = json.load(f)
            print("âœ“ æˆåŠŸåŠ è½½é¢„è¯„åˆ†æ—¥å¿—")
        except Exception as e:
            print(f"âœ— æ— æ³•åŠ è½½é¢„è¯„åˆ†æ—¥å¿—: {e}")

        # é¢„æµ‹æ—¥å¿—
        try:
            with open(
                os.path.join(self.log_dir, "Adaptive_prediction_log.json"), "r"
            ) as f:
                self.prediction_log = json.load(f)
            print("âœ“ æˆåŠŸåŠ è½½é¢„æµ‹æ—¥å¿—")
        except Exception as e:
            print(f"âœ— æ— æ³•åŠ è½½é¢„æµ‹æ—¥å¿—: {e}")

        # æœ€ä½³å›å½’æ¨¡å‹æ—¥å¿—
        try:
            with open(
                os.path.join(self.log_dir, "Adaptive_best_regression_log.json"), "r"
            ) as f:
                self.best_regression = json.load(f)
            print("âœ“ æˆåŠŸåŠ è½½æœ€ä½³å›å½’æ¨¡å‹æ—¥å¿—")
        except Exception as e:
            print(f"âœ— æ— æ³•åŠ è½½æœ€ä½³å›å½’æ¨¡å‹æ—¥å¿—: {e}")

        # æœ€ä½³é¢„è¯„åˆ†æ—¥å¿—
        try:
            with open(
                os.path.join(self.log_dir, "Adaptive_best_prescoring_log.json"), "r"
            ) as f:
                self.best_prescoring = json.load(f)
            print("âœ“ æˆåŠŸåŠ è½½æœ€ä½³é¢„è¯„åˆ†æ—¥å¿—")
        except Exception as e:
            print(f"âœ— æ— æ³•åŠ è½½æœ€ä½³é¢„è¯„åˆ†æ—¥å¿—: {e}")

    def preprocess_data(self):
        """é¢„å¤„ç†å’Œæ•´åˆæ—¥å¿—æ•°æ®ï¼Œä¿æŒè‡ªç¯æ¨¡å‹å’ŒçœŸå®è¾¹æ¨¡å‹çš„é…å¯¹å…³ç³»"""
        # åˆ›å»ºåŒ…å«æ‰€æœ‰æ¨¡å‹æ€§èƒ½æ•°æ®çš„DataFrame
        models_data = []

        # æ·»åŠ æƒé‡æ³•çš„ç»“æœ
        weights_method_results = {}
        weights_method_train_scores = {}

        # è·å–æƒé‡æ³•çš„é¢„æµ‹åˆ†æ•°
        for fold_num in self.prediction_log.keys():
            if "weights_method" in self.prediction_log[fold_num]:
                weights_method_results[fold_num] = self.prediction_log[fold_num][
                    "weights_method"
                ]

        # è·å–æƒé‡æ³•çš„è®­ç»ƒåˆ†æ•°
        for fold_num in self.best_prescoring.keys():
            best_method = self.best_prescoring[fold_num]
            if best_method in self.prescoring_log.get(fold_num, {}):
                weights_method_train_scores[fold_num] = self.prescoring_log[fold_num][
                    best_method
                ]

        # è®°å½•è¿‡æ»¤çŠ¶æ€
        filtered_stats = {
            "unconverged_models": 0,  # è®­ç»ƒé˜¶æ®µæœªæ”¶æ•›å¥½çš„æ¨¡å‹æ•°
            "underperforming_models": 0,  # é¢„æµ‹é˜¶æ®µè¡¨ç°ä¸ä½³çš„æ¨¡å‹æ•°
            "filtered_due_to_paired_model": 0,  # å› ä¸ºé…å¯¹æ¨¡å‹è¢«è¿‡æ»¤è€Œä¸€åŒè¿‡æ»¤çš„æ¨¡å‹æ•°
        }

        # å…ˆæ”¶é›†æ‰€æœ‰æ¨¡å‹çš„æ•°æ®ï¼ˆä¸åšè¿‡æ»¤ï¼‰
        all_models_by_fold = {}

        for fold_num in self.regression_log.keys():
            # æ·»åŠ æƒé‡æ³•çš„ç»“æœ
            weights_model = {
                "fold": fold_num,
                "model_name": "weights_method",
                "train_score": weights_method_train_scores.get(fold_num),
                "test_score": weights_method_results.get(fold_num),
                "is_best": False,
                "model_category": "baseline_weights",
            }

            models_data.append(weights_model)

            # åˆå§‹åŒ–å½“å‰foldçš„æ¨¡å‹åˆ—è¡¨
            fold_models = []

            # è·å–å½“å‰foldçš„æƒé‡æ–¹æ³•åŸºçº¿å¾—åˆ†
            fold_weight_train_score = weights_method_train_scores.get(fold_num)
            fold_weight_test_score = weights_method_results.get(fold_num)

            for model_name, train_score in self.regression_log[fold_num].items():
                # è·å–æµ‹è¯•åˆ†æ•°
                test_score = self.prediction_log.get(fold_num, {}).get(model_name, None)

                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
                is_best = model_name == self.best_regression.get(fold_num, None)

                # è§£ææ¨¡å‹åç§°ä»¥æå–å‚æ•°
                parsed_params = self.parse_model_name(model_name)

                # åˆå¹¶æ‰€æœ‰æ•°æ®
                model_info = {
                    "fold": fold_num,
                    "model_name": model_name,
                    "train_score": train_score,
                    "test_score": test_score,
                    "is_best": is_best,
                    "should_filter": False,  # æ ‡è®°æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤
                    "filter_reason": None,  # è¿‡æ»¤åŸå› 
                    **parsed_params,
                }

                # æ£€æŸ¥è¿‡æ»¤æ¡ä»¶1ï¼šå¦‚æœè®­ç»ƒé˜¶æ®µå¾—åˆ†ä½äºæœ€ä½³æƒé‡æ–¹æ³•(æœªæ­£ç¡®æ”¶æ•›)
                if (
                    fold_weight_train_score is not None
                    and train_score < fold_weight_train_score
                ):
                    # model_info["should_filter"] = True
                    model_info["filter_reason"] = "unconverged"
                    # filtered_stats["unconverged_models"] += 1

                # æ£€æŸ¥è¿‡æ»¤æ¡ä»¶2ï¼šå¦‚æœé¢„æµ‹é˜¶æ®µå¾—åˆ†ä½äºæƒé‡æ³•(è¡¨ç°ä¸ä½³)
                if (
                    fold_weight_test_score is not None
                    and test_score is not None
                    and test_score < fold_weight_test_score
                ):
                    model_info["should_filter"] = True
                    model_info["filter_reason"] = "underperforming"
                    filtered_stats["underperforming_models"] += 1

                fold_models.append(model_info)

            all_models_by_fold[fold_num] = fold_models

        # æ‰¾å‡ºè‡ªç¯æ¨¡å‹å’ŒçœŸå®è¾¹æ¨¡å‹çš„é…å¯¹å…³ç³»
        paired_models = {}

        for fold_num, fold_models in all_models_by_fold.items():
            fold_paired = {}

            # å…ˆæŒ‰ä¸åŒç±»åˆ«åˆ†ç»„
            selfloop_models = [
                m for m in fold_models if m.get("model_category") == "gat_selfloop"
            ]
            realedge_models = [
                m for m in fold_models if m.get("model_category") == "gat_realedge"
            ]

            # é…å¯¹è‡ªç¯æ¨¡å‹å’ŒçœŸå®è¾¹æ¨¡å‹
            for selfloop_model in selfloop_models:
                for realedge_model in realedge_models:
                    # æ£€æŸ¥æ˜¯å¦å…·æœ‰ç›¸åŒçš„è¶…å‚æ•°é…ç½®
                    if (
                        selfloop_model["loss_type"] == realedge_model["loss_type"]
                        and selfloop_model["hidden_dim"] == realedge_model["hidden_dim"]
                        and selfloop_model["penalty"] == realedge_model["penalty"]
                        and selfloop_model["alpha"] == realedge_model["alpha"]
                        and selfloop_model["dropout"] == realedge_model["dropout"]
                        and selfloop_model["learning_rate"]
                        == realedge_model["learning_rate"]
                        and selfloop_model["heads"] == realedge_model["heads"]
                    ):

                        # è®°å½•é…å¯¹å…³ç³»
                        fold_paired[selfloop_model["model_name"]] = realedge_model[
                            "model_name"
                        ]
                        fold_paired[realedge_model["model_name"]] = selfloop_model[
                            "model_name"
                        ]
                        break

            paired_models[fold_num] = fold_paired

        # æ ¹æ®é…å¯¹å…³ç³»åº”ç”¨è¿‡æ»¤è§„åˆ™
        for fold_num, fold_models in all_models_by_fold.items():
            fold_paired = paired_models[fold_num]

            # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹çš„é…å¯¹æ¨¡å‹æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤
            for model in fold_models:
                paired_model_name = fold_paired.get(model["model_name"])

                if paired_model_name and not model["should_filter"]:
                    # æ‰¾åˆ°é…å¯¹çš„æ¨¡å‹
                    paired_model = next(
                        (
                            m
                            for m in fold_models
                            if m["model_name"] == paired_model_name
                        ),
                        None,
                    )

                    # å¦‚æœé…å¯¹æ¨¡å‹åº”è¯¥è¢«è¿‡æ»¤ï¼Œåˆ™è¿™ä¸ªæ¨¡å‹ä¹Ÿåº”è¯¥è¢«è¿‡æ»¤
                    if paired_model and paired_model["should_filter"]:
                        # model["should_filter"] = True
                        model["filter_reason"] = (
                            f"paired_with_{paired_model['filter_reason']}"
                        )
                        filtered_stats["filtered_due_to_paired_model"] += 1

            # åªä¿ç•™ä¸åº”è¯¥è¢«è¿‡æ»¤çš„æ¨¡å‹
            valid_fold_models = [m for m in fold_models if not m["should_filter"]]

            # å°†æœ‰æ•ˆæ¨¡å‹æ·»åŠ åˆ°æœ€ç»ˆæ•°æ®åˆ—è¡¨ä¸­
            for model in valid_fold_models:
                # ç§»é™¤ä¸´æ—¶è¿‡æ»¤æ ‡è®°
                if "should_filter" in model:
                    del model["should_filter"]
                if "filter_reason" in model:
                    del model["filter_reason"]

                models_data.append(model)

        # åˆ›å»ºDataFrame
        self.model_performance_df = pd.DataFrame(models_data)

        # è®°å½•é…å¯¹å…³ç³»ä»¥ä¾›åç»­åˆ†æ
        self.paired_models = paired_models

        # æ‰“å°è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
        total_filtered = (
            filtered_stats["unconverged_models"]
            + filtered_stats["underperforming_models"]
            + filtered_stats["filtered_due_to_paired_model"]
        )

        print(f"âœ“ æˆåŠŸå¤„ç† {len(self.model_performance_df)} ä¸ªæ¨¡å‹é…ç½®çš„æ•°æ®")
        print(f"ğŸ” è¿‡æ»¤æ‰ {total_filtered} ä¸ªæ¨¡å‹:")
        print(f"  - {filtered_stats['unconverged_models']} ä¸ªæ¨¡å‹è®­ç»ƒé˜¶æ®µæœªæ­£ç¡®æ”¶æ•›")
        print(
            f"  - {filtered_stats['underperforming_models']} ä¸ªæ¨¡å‹é¢„æµ‹è¡¨ç°ä¸å¦‚æƒé‡æ³•"
        )
        print(
            f"  - {filtered_stats['filtered_due_to_paired_model']} ä¸ªæ¨¡å‹å› é…å¯¹æ¨¡å‹è¢«è¿‡æ»¤è€Œä¸€åŒç§»é™¤"
        )

        # è®°å½•è¿‡æ»¤ç»“æœ
        self.filtered_stats = filtered_stats

        return filtered_stats

    def parse_model_name(self, model_name: str) -> Dict[str, Any]:
        """
        è§£ææ¨¡å‹åç§°ï¼Œæå–å‚æ•°ä¿¡æ¯

        å‚æ•°:
            model_name: æ¨¡å‹åç§°å­—ç¬¦ä¸²

        è¿”å›:
            åŒ…å«å‚æ•°ä¿¡æ¯çš„å­—å…¸
        """
        params = {}
        parts = model_name.split("_")

        # åŸºæœ¬ä¿¡æ¯
        params["model_type"] = parts[0]
        params["loss_type"] = parts[1]
        params["hidden_dim"] = int(parts[2])
        params["penalty"] = parts[3]
        params["heads"] = parts[4]
        if parts[4] == "nohead":
            params["model_category"] = "baseline_mlp"

        # è§£æå…¶ä»–è¶…å‚æ•°
        i = 5
        while i < len(parts):
            if parts[i].startswith("a"):
                params["alpha"] = float(parts[i].replace("a", ""))
            elif parts[i].startswith("dr"):
                params["dropout"] = float(parts[i].replace("dr", ""))
            elif parts[i].startswith("lr"):
                params["learning_rate"] = float(parts[i].replace("lr", ""))
            elif parts[i] == "selfloop":
                params["use_self_loops"] = True
            elif parts[i] == "shuf":
                params["shuffle"] = True
            i += 1

        # è®¾ç½®é»˜è®¤å€¼
        params.setdefault("use_self_loops", False)
        params.setdefault("shuffle", False)

        if "model_category" not in params:
            if params["use_self_loops"]:
                params["model_category"] = "gat_selfloop"  # GAT+è‡ªç¯æ¨¡å‹
            else:
                params["model_category"] = "gat_realedge"  # GAT+çœŸå®è¾¹æ¨¡å‹

        return params

    def analyze_overall_performance(self):
        """åˆ†ææ•´ä½“æ€§èƒ½å¹¶ç”Ÿæˆæ‘˜è¦"""
        # è·å–æ ‡è®°ä¸ºæœ€ä½³çš„æ¨¡å‹
        best_models = self.model_performance_df[self.model_performance_df["is_best"]]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ€ä½³æ¨¡å‹(å¯èƒ½æ˜¯è¢«è¿‡æ»¤æ‰äº†)ï¼Œå¤„ç†è¿™ç§æƒ…å†µ
        if best_models.empty:
            print(
                "âš  è­¦å‘Š: æ‰€æœ‰æ ‡è®°ä¸º'æœ€ä½³'çš„æ¨¡å‹éƒ½è¢«è¿‡æ»¤æ‰äº†ï¼Œä½¿ç”¨å‰©ä½™æ¨¡å‹ä¸­è¡¨ç°æœ€å¥½çš„ä½œä¸ºæ›¿ä»£"
            )

            # æŒ‰ç…§foldåˆ†ç»„ï¼Œä¸ºæ¯ä¸ªfoldé€‰æ‹©ä¸€ä¸ªæ–°çš„æœ€ä½³æ¨¡å‹
            for fold in self.model_performance_df["fold"].unique():
                fold_models = self.model_performance_df[
                    self.model_performance_df["fold"] == fold
                ]

                # å…ˆå°è¯•ä»éæƒé‡æ¨¡å‹ä¸­é€‰æ‹©
                fold_nn_models = fold_models[
                    fold_models["model_category"] != "baseline_weights"
                ]

                if not fold_nn_models.empty:
                    # æ‰¾å‡ºè®­ç»ƒå¾—åˆ†æœ€é«˜çš„éæƒé‡æ¨¡å‹å¹¶æ ‡è®°ä¸ºæœ€ä½³
                    best_idx = fold_nn_models["train_score"].idxmax()
                    self.model_performance_df.loc[best_idx, "is_best"] = True
                    print(
                        f"  æŠ˜ {fold}: æ ‡è®° {self.model_performance_df.loc[best_idx, 'model_name']} ä¸ºæ–°çš„æœ€ä½³æ¨¡å‹"
                    )
                else:
                    print(f"  æŠ˜ {fold}: æ²¡æœ‰æ‰¾åˆ°éæƒé‡æ¨¡å‹")

            # é‡æ–°è·å–æ›´æ–°åçš„æœ€ä½³æ¨¡å‹åˆ—è¡¨
            best_models = self.model_performance_df[
                self.model_performance_df["is_best"]
            ]

        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary = {
            "æ€»æ¨¡å‹æ•°": len(self.model_performance_df),
            "è¿‡æ»¤æ‰çš„æ¨¡å‹æ•°": self.filtered_stats.get("unconverged_models", 0)
            + self.filtered_stats.get("underperforming_models", 0),
            "æœ€ä½³æ¨¡å‹fold": best_models["fold"].tolist(),
            "æœ€ä½³æ¨¡å‹": best_models["model_name"].tolist(),
            "æœ€ä½³æ¨¡å‹è®­ç»ƒå¾—åˆ†": best_models["train_score"].tolist(),
            "æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†": best_models["test_score"].tolist(),
        }

        # ä¿å­˜æ‘˜è¦
        with open(os.path.join(self.output_dir, "performance_summary.json"), "w") as f:
            json.dump(summary, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)

        # æ‰“å°æ‘˜è¦
        print("\n===== æ€§èƒ½æ‘˜è¦ =====")
        print(f"æ€»æ¨¡å‹æ•°: {summary['æ€»æ¨¡å‹æ•°']}")

        for i, model in enumerate(summary["æœ€ä½³æ¨¡å‹"]):
            print(f"\næŠ˜ {summary['æœ€ä½³æ¨¡å‹fold'][i]}:")
            print(f"  æœ€ä½³æ¨¡å‹: {model}")
            print(f"  è®­ç»ƒå¾—åˆ†: {summary['æœ€ä½³æ¨¡å‹è®­ç»ƒå¾—åˆ†'][i]:.4f}")
            print(f"  æµ‹è¯•å¾—åˆ†: {summary['æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†'][i]:.4f}")

        return summary

    def analyze_parameter_impact(self):
        """åˆ†æä¸åŒå‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“"""
        # è¦åˆ†æçš„å‚æ•°åˆ—è¡¨
        params_to_analyze = [
            "heads",
            "loss_type",
            "hidden_dim",
            "penalty",
            "learning_rate",
            "alpha",
            "dropout",
        ]

        # åˆ›å»ºå‚æ•°å½±å“åˆ†æç›®å½•
        param_dir = os.path.join(self.output_dir, "parameter_impact")
        os.makedirs(param_dir, exist_ok=True)

        # åˆ†ææ¯ä¸ªå‚æ•°
        param_impact = {}

        nn_models_df = self.model_performance_df[
            self.model_performance_df["model_category"] != "baseline_weights"
        ]

        for param in params_to_analyze:
            if param not in nn_models_df.columns:
                continue

            unique_values = nn_models_df[param].unique()
            if len(unique_values) <= 1:
                continue

            # è®¡ç®—æ¯ä¸ªå‚æ•°å€¼çš„å¹³å‡æ€§èƒ½
            impact_data = []
            unique_values = nn_models_df[param].unique()

            for value in unique_values:
                subset = nn_models_df[nn_models_df[param] == value]

                impact_data.append(
                    {
                        "param_value": value,
                        "count": len(subset),
                        "mean_train": subset["train_score"].mean(),
                        "mean_test": subset["test_score"].mean(),
                        "best_count": subset["is_best"].sum(),
                    }
                )

            # ä¿å­˜å‚æ•°å½±å“æ•°æ®
            param_impact[param] = impact_data

            # åˆ›å»ºå‚æ•°å½±å“å›¾è¡¨
            self.plot_parameter_impact(param, impact_data, param_dir)

        # ä¿å­˜å‚æ•°å½±å“æ‘˜è¦
        with open(
            os.path.join(self.output_dir, "parameter_impact_summary.json"), "w"
        ) as f:
            json.dump(param_impact, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)

        return param_impact

    def plot_parameter_impact(
        self, param: str, impact_data: List[Dict], output_dir: str
    ):
        """
        ç»˜åˆ¶å‚æ•°å½±å“å›¾è¡¨

        å‚æ•°:
            param: å‚æ•°åç§°
            impact_data: å½±å“æ•°æ®åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        # è½¬æ¢ä¸ºDataFrame
        impact_df = pd.DataFrame(impact_data)
        impact_df["param_label"] = impact_df["param_value"].astype(str)

        # æ’åº
        if param in ["hidden_dim", "learning_rate", "alpha", "dropout"]:
            impact_df = impact_df.sort_values(by="param_value")

        # ç»˜åˆ¶è¿‡æ‹Ÿåˆå¯¹æ¯”å›¾
        plt.figure(figsize=(12, 6))

        x = np.arange(len(impact_df))
        width = 0.35

        # è®­ç»ƒå’Œæµ‹è¯•åˆ†æ•°
        plt.bar(x - width / 2, impact_df["mean_train"], width, label="è®­ç»ƒå¾—åˆ†")
        plt.bar(x + width / 2, impact_df["mean_test"], width, label="æµ‹è¯•å¾—åˆ†")

        # æ·»åŠ æ ‡ç­¾
        plt.xlabel(param)
        plt.ylabel("å¹³å‡å¾—åˆ†(MAP)")
        plt.title(f"{param}å‚æ•°å¯¹è®­ç»ƒå’Œæµ‹è¯•æ€§èƒ½çš„å¯¹æ¯”")
        plt.xticks(x, impact_df["param_label"])
        plt.legend()
        plt.grid(axis="y", linestyle="--", alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"{param}_train_test_compare.png"))
        plt.close()

    def analyze_self_loops_effect(self):
        """åˆ†æè‡ªç¯æ¨¡å¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç›´æ¥ä½¿ç”¨é¢„å¤„ç†é˜¶æ®µå»ºç«‹çš„é…å¯¹å…³ç³»"""
        # ç¡®ä¿model_categoryå­—æ®µå­˜åœ¨
        if "model_category" not in self.model_performance_df.columns:
            print("âš  æ•°æ®ä¸­æ²¡æœ‰model_categoryä¿¡æ¯ï¼Œæ— æ³•ç»˜åˆ¶åˆ†ç±»æ¨¡å‹å¯¹æ¯”å›¾")
            return None

        # åˆå§‹åŒ–ç»“æœç»“æ„
        self_loop_stats = {}

        # æŒ‰foldåˆ†æ
        for fold in self.model_performance_df["fold"].unique():
            # è·å–å½“å‰foldçš„æ¨¡å‹
            fold_models = self.model_performance_df[
                self.model_performance_df["fold"] == fold
            ]
            fold_self_loop = fold_models[
                fold_models["model_category"] == "gat_selfloop"
            ]
            fold_real_edge = fold_models[
                fold_models["model_category"] == "gat_realedge"
            ]

            # ä½¿ç”¨é¢„å¤„ç†é˜¶æ®µå·²å»ºç«‹çš„é…å¯¹å…³ç³»
            fold_pairs = []

            # æ£€æŸ¥æ˜¯å¦æœ‰é…å¯¹ä¿¡æ¯
            for _, self_loop_model in fold_self_loop.iterrows():
                model_name = self_loop_model["model_name"]

                # è·å–å¯¹åº”çš„çœŸå®è¾¹æ¨¡å‹åç§°
                if model_name in self.paired_models[fold]:
                    real_edge_name = self.paired_models[fold][model_name]

                    # æŸ¥æ‰¾å¯¹åº”çš„çœŸå®è¾¹æ¨¡å‹
                    real_edge_models = fold_real_edge[
                        fold_real_edge["model_name"] == real_edge_name
                    ]

                    if not real_edge_models.empty:
                        real_edge_model = real_edge_models.iloc[0]

                        pair_info = {
                            "self_loop_model": model_name,
                            "real_edge_model": real_edge_name,
                            "self_loop_train": self_loop_model["train_score"],
                            "real_edge_train": real_edge_model["train_score"],
                            "self_loop_test": self_loop_model["test_score"],
                            "real_edge_test": real_edge_model["test_score"],
                            "real_edge_improvement": real_edge_model["test_score"]
                            - self_loop_model["test_score"],
                        }

                        fold_pairs.append(pair_info)

            # å¯¹ç»“æœæŒ‰çœŸå®è¾¹æ¨¡å‹å¯¹æ¯”è‡ªç¯æ¨¡å‹çš„æå‡é‡é™åºæ’åˆ—
            fold_pairs.sort(key=lambda x: x["real_edge_improvement"], reverse=True)

            # ä¿å­˜å½“å‰foldçš„ç»Ÿè®¡ä¿¡æ¯
            self_loop_stats[fold] = fold_pairs

        # ä¿å­˜è‡ªç¯åˆ†æç»“æœ
        with open(os.path.join(self.output_dir, "self_loops_analysis.json"), "w") as f:
            json.dump(
                self_loop_stats, f, indent=4, ensure_ascii=False, cls=NumpyEncoder
            )

        # ç»˜åˆ¶ä¸åŒç±»åˆ«æ¨¡å‹çš„å¯¹æ¯”æ•£ç‚¹å›¾
        self.plot_model_categories_comparison(self_loop_stats)

        return self_loop_stats

    def plot_model_categories_comparison(self, fold_comparisons):
        """
        ç»˜åˆ¶ä¸åŒç±»åˆ«æ¨¡å‹çš„å¯¹æ¯”æ•£ç‚¹å›¾(MLP, GATè‡ªç¯, GATçœŸå®è¾¹)

        å‚æ•°:
            fold_comparisons: æŒ‰foldç»„ç»‡çš„æ¯”è¾ƒæ•°æ®å­—å…¸
        """
        if not fold_comparisons:
            return

        # å‡†å¤‡æ•°æ®
        all_models = self.model_performance_df.copy()

        # åˆ›å»ºæ•£ç‚¹å›¾
        plt.figure(figsize=(12, 10))

        # ä¸ºæ¯ä¸ªfoldåˆ†é…ä¸åŒé¢œè‰²
        folds = all_models["fold"].unique()
        colors = plt.cm.tab10(np.linspace(0, 1, len(folds)))
        fold_color_map = {fold: colors[i] for i, fold in enumerate(folds)}

        legend_elements = []

        # ä¸ºæ¯ç§æ¨¡å‹ç±»åˆ«å®šä¹‰æ ‡è®°
        markers = {
            "baseline_mlp": "^",  # ä¸‰è§’å½¢
            "gat_selfloop": "s",  # æ–¹å½¢
            "gat_realedge": "o",  # åœ†å½¢
        }

        # æ¨¡å‹ç±»åˆ«åç§°æ˜ å°„
        category_names = {
            "baseline_mlp": "MLPæ¨¡å‹",
            "gat_selfloop": "GATè‡ªç¯æ¨¡å‹",
            "gat_realedge": "GATçœŸå®è¾¹æ¨¡å‹",
        }

        # ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹ç‚¹
        for fold in folds:
            fold_models = all_models[all_models["fold"] == fold]

            # æŒ‰æ¨¡å‹ç±»åˆ«ç»˜åˆ¶
            for category, marker in markers.items():
                category_models = fold_models[fold_models["model_category"] == category]

                if not category_models.empty:
                    plt.scatter(
                        category_models["train_score"],
                        category_models["test_score"],
                        marker=marker,
                        s=80,
                        color=fold_color_map[fold],
                        alpha=0.7,
                        label="_nolegend_",
                    )

                    # åªä¸ºæ¯ä¸ªç±»åˆ«çš„ç¬¬ä¸€ä¸ªfoldæ·»åŠ å›¾ä¾‹
                    if fold == folds[0]:
                        legend_elements.append(
                            Line2D(
                                [0],
                                [0],
                                marker=marker,
                                color="k",
                                linestyle="none",
                                markerfacecolor="none",
                                markeredgewidth=1.5,
                                label=f"{category_names[category]}",
                                markersize=10,
                            )
                        )

            legend_elements.append(
                Line2D(
                    [0],
                    [0],
                    marker="o",
                    color="w",
                    label=f"æŠ˜ {fold}",
                    markerfacecolor=fold_color_map[fold],
                    markersize=10,
                )
            )

        # æ ‡è®°æœ€ä½³æ¨¡å‹
        best_models = all_models[all_models["is_best"]]
        legend_elements.append(
            Line2D(
                [0],
                [0],
                marker="o",
                color="w",
                markeredgecolor="red",
                markerfacecolor="none",
                markersize=12,
                label="æœ€ä½³æ¨¡å‹",
                markeredgewidth=2,
            )
        )
        for _, model in best_models.iterrows():
            category = model["model_category"]
            marker = markers[category]
            plt.scatter(
                model["train_score"],
                model["test_score"],
                marker=marker,
                s=120,
                facecolors="none",
                edgecolors="red",
                linewidths=2,
                label="_nolegend_",
            )

        # ç”¨è™šçº¿è¿æ¥é…å¯¹çš„æ¨¡å‹ - ä½¿ç”¨æŒ‰foldç»„ç»‡çš„é…å¯¹æ¨¡å‹
        for fold, fold_data in fold_comparisons.items():
            for pair in fold_data:
                self_loop_model = all_models[
                    (all_models["model_name"] == pair["self_loop_model"])
                    & (all_models["fold"] == fold)
                ]
                real_edge_model = all_models[
                    (all_models["model_name"] == pair["real_edge_model"])
                    & (all_models["fold"] == fold)
                ]

                if not self_loop_model.empty and not real_edge_model.empty:
                    plt.plot(
                        [
                            self_loop_model.iloc[0]["train_score"],
                            real_edge_model.iloc[0]["train_score"],
                        ],
                        [
                            self_loop_model.iloc[0]["test_score"],
                            real_edge_model.iloc[0]["test_score"],
                        ],
                        "k--",  # é»‘è‰²è™šçº¿
                        alpha=0.5,
                        linewidth=0.7,
                    )

        plt.axis("equal")

        # æ·»åŠ è¾…åŠ©å…ƒç´ 
        plt.xlabel("å›å½’é˜¶æ®µå¾—åˆ†", fontsize=12)
        plt.ylabel("æµ‹è¯•é˜¶æ®µå¾—åˆ†", fontsize=12)
        plt.title("ä¸åŒç±»åˆ«æ¨¡å‹æ€§èƒ½å¯¹æ¯”", fontsize=14)

        plt.legend(handles=legend_elements, loc="best")

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "model_categories_comparison.png"))
        plt.close()

    def analyze_training_time(self):
        """åˆ†æè®­ç»ƒæ—¶é—´"""
        if not self.training_time:
            eprint("âš  æ²¡æœ‰è®­ç»ƒæ—¶é—´æ•°æ®ï¼Œè·³è¿‡åˆ†æ")
            return None

        # è®¡ç®—å¹³å‡æ¯bugå’Œæ¯æ–‡ä»¶çš„è®­ç»ƒæ—¶é—´
        time_stats = {}

        for fold, data in self.training_time.items():
            time_stats[fold] = {
                "total_time_seconds": data["time"],
                "total_time_minutes": data["time"] / 60,
                "total_time_hours": data["time"] / 3600,
                "bugs_count": data["bugs"],
                "files_count": data["files"],
                "seconds_per_bug": (
                    data["time"] / data["bugs"] if data["bugs"] > 0 else 0
                ),
                "seconds_per_file": (
                    data["time"] / data["files"] if data["files"] > 0 else 0
                ),
            }

        # ä¿å­˜ç»“æœ
        with open(
            os.path.join(self.output_dir, "training_time_analysis.json"), "w"
        ) as f:
            json.dump(time_stats, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)

        # æ‰“å°æ—¶é—´ç»Ÿè®¡
        print("\n===== è®­ç»ƒæ—¶é—´åˆ†æ =====")
        for fold, stats in time_stats.items():
            print(f"æŠ˜ {fold}:")
            print(
                f"  æ€»è®­ç»ƒæ—¶é—´: {stats['total_time_hours']:.2f}å°æ—¶ ({stats['total_time_minutes']:.2f}åˆ†é’Ÿ)"
            )
            print(f"  Bugæ•°é‡: {stats['bugs_count']}")
            print(f"  æ–‡ä»¶æ•°é‡: {stats['files_count']}")
            print(f"  å¹³å‡æ¯bugè®­ç»ƒæ—¶é—´: {stats['seconds_per_bug']:.2f}ç§’")
            print(f"  å¹³å‡æ¯æ–‡ä»¶è®­ç»ƒæ—¶é—´: {stats['seconds_per_file']:.2f}ç§’")

        return time_stats

    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š"""

        # è·å–æœ€ä½³æ¨¡å‹è¯¦æƒ…
        best_models = self.model_performance_df[self.model_performance_df["is_best"]]
        best_model_category = best_models["model_category"].iloc[0]

        # æŒ‰æ¨¡å‹ç±»åˆ«åˆ†ç»„è®¡ç®—æœ€ä½³æ€§èƒ½
        fold_category_performance = (
            self.model_performance_df.groupby(["fold", "model_category"])["test_score"]
            .max()
            .reset_index()
        )

        # è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­ä¸€ä¸ªæ¨¡å‹ç±»åˆ«æ˜¯å¦åœ¨æ‰€æœ‰foldä¸­éƒ½ä¼˜äºå¦ä¸€ä¸ªç±»åˆ«
        def is_better_in_all_folds(category1, category2):
            """åˆ¤æ–­category1æ˜¯å¦åœ¨æ‰€æœ‰foldä¸­éƒ½ä¼˜äºcategory2"""
            for fold in fold_category_performance["fold"].unique():
                # è·å–å½“å‰foldä¸­ä¸¤ç§ç±»åˆ«çš„æœ€é«˜å¾—åˆ†
                cat1_score = fold_category_performance[
                    (fold_category_performance["fold"] == fold)
                    & (fold_category_performance["model_category"] == category1)
                ]["test_score"]

                cat2_score = fold_category_performance[
                    (fold_category_performance["fold"] == fold)
                    & (fold_category_performance["model_category"] == category2)
                ]["test_score"]

                # å¦‚æœä»»ä¸€ç±»åˆ«åœ¨è¯¥foldä¸­æ²¡æœ‰æ•°æ®ï¼Œæˆ–cat1ä¸ä¼˜äºcat2ï¼Œåˆ™è¿”å›False
                if (
                    cat1_score.empty
                    or cat2_score.empty
                    or cat1_score.iloc[0] <= cat2_score.iloc[0]
                ):
                    return False

            return True

        # è¾…åŠ©å‡½æ•°ï¼šæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ç±»åˆ«å¹¶è¿”å›ç»“è®º
        def compare_categories(category, name):
            """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ç±»åˆ«ï¼Œè¿”å›æ¯”è¾ƒç»“è®º"""
            if category not in fold_category_performance["model_category"].values:
                return f"æœªæ¯”è¾ƒGATæ¨¡å‹ä¸{name}"

            if is_better_in_all_folds("gat_realedge", category):
                return f"GATæ¨¡å‹è¡¨ç°ä¼˜äº{name}"
            elif is_better_in_all_folds(category, "gat_realedge"):
                return f"{name}è¡¨ç°ä¼˜äºGATæ¨¡å‹"
            else:
                return f"GATæ¨¡å‹ä¸{name}è¡¨ç°æ— æ³•ç¡®å®šæ˜æ˜¾ä¼˜åŠ£"

        # æ‰§è¡Œå„ç§æ¯”è¾ƒ
        findings = [
            compare_categories("baseline_weights", "æƒé‡æ³•"),
            compare_categories("gat_selfloop", "ä»…è‡ªç¯GATæ¨¡å‹"),
            compare_categories("baseline_mlp", "MLPæ¨¡å‹"),
        ]

        # æ•´ä½“æ€§èƒ½ç»Ÿè®¡
        category_performance = (
            self.model_performance_df.groupby("model_category")["test_score"]
            .agg(["max", "mean"])
            .reset_index()
        )

        # å‡†å¤‡æŠ¥å‘Šå†…å®¹
        report = {
            "æ€»ç»“": {
                "è¯„ä¼°æ—¶é—´": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
                "è¯„ä¼°ç›®å½•": self.log_dir,
                "æ¨¡å‹æ€»æ•°": len(self.model_performance_df),
                "æœ€ä½³æ¨¡å‹": best_models["model_name"].tolist(),
                "æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†": best_models["test_score"].tolist(),
                "æœ€ä½³æ¨¡å‹ç±»å‹": {
                    "baseline_mlp": "åŸºçº¿MLPæ¨¡å‹",
                    "baseline_weights": "æƒé‡çº¿æ€§ç»„åˆæ¨¡å‹",
                    "gat_realedge": "GATæ¨¡å‹",
                    "gat_selfloop": "ä»…è‡ªç¯GATæ¨¡å‹",
                }.get(best_model_category, "æœªçŸ¥ç±»å‹"),
            },
            "æ¨¡å‹ç±»åˆ«æ€§èƒ½": category_performance.to_dict(orient="records"),
            "ä¸»è¦å‘ç°": findings,
        }

        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        with open(os.path.join(self.output_dir, "final_report.json"), "w") as f:
            json.dump(report, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)

        # åˆ›å»ºå¯è¯»æ€§æ›´å¥½çš„æ–‡æœ¬æŠ¥å‘Š
        report_text = f"""
=========================================
è‡ªé€‚åº”GATè®­ç»ƒè¯„ä¼°æŠ¥å‘Š
=========================================
è¯„ä¼°æ—¶é—´: {report['æ€»ç»“']['è¯„ä¼°æ—¶é—´']}
è¯„ä¼°ç›®å½•: {report['æ€»ç»“']['è¯„ä¼°ç›®å½•']}
æ¨¡å‹æ€»æ•°: {report['æ€»ç»“']['æ¨¡å‹æ€»æ•°']}

æœ€ä½³æ¨¡å‹: {report['æ€»ç»“']['æœ€ä½³æ¨¡å‹'][0]}
æœ€ä½³æ¨¡å‹ç±»å‹: {report['æ€»ç»“']['æœ€ä½³æ¨¡å‹ç±»å‹']}
æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†: {report['æ€»ç»“']['æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†'][0]:.4f}

ä¸»è¦å‘ç°:
{chr(10).join('- ' + finding for finding in report['ä¸»è¦å‘ç°'])}
=========================================
        """

        with open(
            os.path.join(self.output_dir, "final_report.txt"), "w", encoding="utf-8"
        ) as f:
            f.write(report_text)

        print("\n" + report_text)

        return report

    def run_all_analyses(self):
        """è¿è¡Œæ‰€æœ‰åˆ†æ"""
        # åŠ è½½æ—¥å¿—æ•°æ®
        self.load_logs()

        # é¢„å¤„ç†æ•°æ®
        self.preprocess_data()

        # è¿è¡Œå„ç§åˆ†æ
        self.analyze_overall_performance()
        self.analyze_parameter_impact()
        self.analyze_self_loops_effect()
        self.analyze_training_time()

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report()

        print(f"\nâœ“ å…¨éƒ¨åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.output_dir}")


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="åˆ†æè‡ªé€‚åº”GATè®­ç»ƒæ—¥å¿—å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
    parser.add_argument("log_dir", help="åŒ…å«è®­ç»ƒæ—¥å¿—æ–‡ä»¶çš„ç›®å½•è·¯å¾„")
    parser.add_argument(
        "--output", "-o", help="è¾“å‡ºç›®å½• (é»˜è®¤ä¸ºlog_dir/analysis_results)"
    )

    args = parser.parse_args()

    # args = argparse.Namespace(
    #     log_dir="tomcat_auto_20250320041613/",
    #     output=None,
    # )

    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œåˆ†æ
    evaluator = AdaptiveGATEvaluator(args.log_dir)

    if args.output:
        evaluator.output_dir = args.output
        os.makedirs(evaluator.output_dir, exist_ok=True)

    evaluator.run_all_analyses()


class NumpyEncoder(json.JSONEncoder):
    """å¤„ç†NumPyç±»å‹çš„JSONç¼–ç å™¨"""

    def default(self, obj):
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):  # æ·»åŠ å¯¹NumPyå¸ƒå°”ç±»å‹çš„å¤„ç†
            return bool(obj)
        return super(NumpyEncoder, self).default(obj)


if __name__ == "__main__":
    main()
