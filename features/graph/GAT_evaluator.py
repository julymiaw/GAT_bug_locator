#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è‡ªé€‚åº”GATè®­ç»ƒè¯„ä¼°è„šæœ¬

ç”¨é€”: åˆ†æè‡ªé€‚åº”è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„å„ç§æ—¥å¿—æ–‡ä»¶ï¼Œæä¾›è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œå¯è§†åŒ–
è¾“å…¥: è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„JSONæ—¥å¿—æ–‡ä»¶
è¾“å‡º: è¯¦ç»†çš„åˆ†æç»“æœã€ç»Ÿè®¡æ•°æ®å’Œå¯è§†åŒ–å›¾è¡¨

ä½œè€…: SRTPå›¢é˜Ÿ
æ—¥æœŸ: 2025å¹´3æœˆ7æ—¥
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
from typing import Dict, List, Any
import argparse
from train_utils import eprint

# Linuxä¸‹è®¾ç½®
mpl.rcParams["font.sans-serif"] = ["SimHei"]  # è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºé»‘ä½“
mpl.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜


class AdaptiveGATEvaluator:
    """è‡ªé€‚åº”GATè®­ç»ƒè¯„ä¼°å™¨ï¼Œç”¨äºåˆ†æè®­ç»ƒæ—¥å¿—å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""

    def __init__(self, log_dir: str):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        å‚æ•°:
            log_dir: åŒ…å«è®­ç»ƒæ—¥å¿—çš„ç›®å½•è·¯å¾„
        """
        self.log_dir = log_dir
        self.output_dir = os.path.join(log_dir, "analysis_results")
        os.makedirs(self.output_dir, exist_ok=True)

        # æ—¥å¿—æ•°æ®
        self.training_time = {}
        self.model_registry = None
        self.model_performance_df = None

        self.paired_models = {}
        self.filtered_stats = {
            "unconverged_models": 0,
            "underperforming_models": 0,
            "filtered_due_to_paired_model": 0,
        }

    def load_logs(self):
        """åŠ è½½æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        print("æ­£åœ¨åŠ è½½æ—¥å¿—æ–‡ä»¶...")

        try:
            with open(
                os.path.join(self.log_dir, "Adaptive_models_registry.json"), "r"
            ) as f:
                registry_data = json.load(f)

                self.model_registry = registry_data
                self.training_time = registry_data.get("training_time", {})
            print("âœ“ æˆåŠŸåŠ è½½å®éªŒè·Ÿè¸ªç®¡ç†å™¨æ¨¡å—")
        except Exception as e:
            print(f"âœ— æ— æ³•åŠ è½½å®éªŒè·Ÿè¸ªç®¡ç†å™¨æ¨¡å—: {e}")
            return False

        return True

    def preprocess_data(self):
        """ç›´æ¥ä»å®éªŒè·Ÿè¸ªç®¡ç†å™¨æ¨¡å—ä¸­æå–å¹¶å¤„ç†æ•°æ®"""
        if not self.model_registry:
            print("âš  å®éªŒè·Ÿè¸ªç®¡ç†å™¨æ¨¡å—æœªåŠ è½½ï¼Œæ— æ³•å¤„ç†æ•°æ®")
            return {}

        models = self.model_registry.get("models", {})

        # æŒ‰foldç»„ç»‡æ•°æ®
        models_by_fold = {}
        weights_methods_by_fold = {}

        # æå–æ‰€æœ‰æ¨¡å‹ä¿¡æ¯
        for model_id, model_data in models.items():
            params = model_data.get("params", {})
            results = model_data.get("results", {})
            fold_num = str(params.get("fold_num", "0"))

            # å‡†å¤‡å­˜å‚¨ä¸åŒfoldçš„æ•°æ®
            if fold_num not in models_by_fold:
                models_by_fold[fold_num] = []

            # å¤„ç†æƒé‡æ–¹æ³•æ¨¡å‹
            if model_id.startswith("weights_method"):
                if fold_num not in weights_methods_by_fold:
                    weights_methods_by_fold[fold_num] = {}

                weights_methods_by_fold[fold_num] = {
                    "fold": fold_num,
                    "model_name": model_id,
                    "train_score": next(
                        (results[k] for k in results if k.startswith("train_")), None
                    ),
                    "test_score": next(
                        (results[k] for k in results if k.startswith("predict_")), None
                    ),
                    "is_best": False,
                    "model_category": "baseline_weights",
                }
                continue

            # å¤„ç†GAT/MLPæ¨¡å‹
            train_score = next(
                (results[k] for k in results if k.startswith("train_")), None
            )
            test_score = next(
                (results[k] for k in results if k.startswith("predict_")), None
            )

            # åˆ¤æ–­æ¨¡å‹ç±»å‹å’Œç±»åˆ«
            if params.get("model_type") == "MLP":
                model_category = "baseline_mlp"
            elif params.get("model_type") == "GAT":
                if params.get("use_self_loops_only", False):
                    model_category = "gat_selfloop"
                else:
                    model_category = "gat_realedge"
            else:
                model_category = "unknown"

            # åˆ›å»ºæ¨¡å‹ä¿¡æ¯å¯¹è±¡
            model_info = {
                "fold": fold_num,
                "model_name": model_id,
                "train_score": train_score,
                "test_score": test_score,
                "is_best": False,  # ç¨åå†ç¡®å®šæœ€ä½³æ¨¡å‹
                "model_category": model_category,
                "loss_type": params.get("loss", "unknown"),
                "hidden_dim": params.get("hidden_dim", 0),
                "penalty": params.get("penalty", "none"),
                "heads": params.get("heads", "nohead"),
                "alpha": params.get("alpha", 0.0),
                "dropout": params.get("dropout", 0.0),
                "learning_rate": params.get("lr", 0.0),
                "use_self_loops": params.get("use_self_loops_only", False),
                "shuffle": params.get("shuffle", False),
            }

            models_by_fold[fold_num].append(model_info)

        # ç¡®å®šæ¯ä¸ªfoldçš„æœ€ä½³æ¨¡å‹å¹¶åº”ç”¨è¿‡æ»¤
        all_models_data = []

        # æ·»åŠ æƒé‡æ³•åˆ°æœ€ç»ˆæ•°æ®
        for fold_num, weights_method in weights_methods_by_fold.items():
            all_models_data.append(weights_method)

        # å¤„ç†å„ä¸ªfoldçš„æ‰€æœ‰æ¨¡å‹
        for fold_num, models in models_by_fold.items():
            # æŒ‰è®­ç»ƒå¾—åˆ†é™åºæ’åº
            models.sort(
                key=lambda x: x["train_score"] if x["train_score"] else 0, reverse=True
            )

            # æ‰¾å‡ºå½“å‰foldçš„æœ€ä½³GATæ¨¡å‹
            best_gat = next(
                (m for m in models if m["model_category"] == "gat_realedge"), None
            )
            if best_gat:
                best_gat["is_best"] = True

            # æƒé‡æ–¹æ³•å¾—åˆ†
            weight_test_score = weights_methods_by_fold[fold_num]["test_score"]

            # æ‰¾å‡ºè‡ªç¯æ¨¡å‹å’Œå®é™…è¾¹æ¨¡å‹çš„é…å¯¹å…³ç³»
            self.paired_models[fold_num] = self._pair_models_in_fold(models)

            # è¿‡æ»¤æ¨¡å‹
            valid_models = []
            for model in models:
                # å¦‚æœæ¨¡å‹æµ‹è¯•å¾—åˆ†ä½äºæƒé‡æ³•ï¼Œæ ‡è®°ä¸ºè¿‡æ»¤
                if (
                    weight_test_score is not None
                    and model["test_score"] is not None
                    and model["test_score"] < weight_test_score
                ):
                    # model["should_filter"] = True
                    self.filtered_stats["underperforming_models"] += 1
                else:
                    model["should_filter"] = False

                # æ£€æŸ¥é…å¯¹æ¨¡å‹çš„è¿‡æ»¤çŠ¶æ€
                paired_model_name = self.paired_models[fold_num].get(
                    model["model_name"]
                )
                if paired_model_name:
                    paired_model = next(
                        (m for m in models if m["model_name"] == paired_model_name),
                        None,
                    )
                    if paired_model and paired_model.get("should_filter", False):
                        # model["should_filter"] = True
                        self.filtered_stats["filtered_due_to_paired_model"] += 1

                # åªä¿ç•™æœªè¿‡æ»¤çš„æ¨¡å‹
                if not model.get("should_filter", False):
                    # ç§»é™¤ä¸´æ—¶æ ‡è®°
                    if "should_filter" in model:
                        del model["should_filter"]
                    valid_models.append(model)

            # æ·»åŠ åˆ°æœ€ç»ˆæ•°æ®
            all_models_data.extend(valid_models)

        # åˆ›å»ºDataFrame
        self.model_performance_df = pd.DataFrame(all_models_data)

        # æ‰“å°è¿‡æ»¤ç»Ÿè®¡
        total_filtered = sum(self.filtered_stats.values())
        print(f"âœ“ æˆåŠŸå¤„ç† {len(self.model_performance_df)} ä¸ªæ¨¡å‹é…ç½®çš„æ•°æ®")
        print(f"ğŸ” è¿‡æ»¤æ‰ {total_filtered} ä¸ªæ¨¡å‹:")
        print(
            f"  - {self.filtered_stats['unconverged_models']} ä¸ªæ¨¡å‹è®­ç»ƒé˜¶æ®µæœªæ­£ç¡®æ”¶æ•›"
        )
        print(
            f"  - {self.filtered_stats['underperforming_models']} ä¸ªæ¨¡å‹é¢„æµ‹è¡¨ç°ä¸å¦‚æƒé‡æ³•"
        )
        print(
            f"  - {self.filtered_stats['filtered_due_to_paired_model']} ä¸ªæ¨¡å‹å› é…å¯¹æ¨¡å‹è¢«è¿‡æ»¤è€Œä¸€åŒç§»é™¤"
        )

        return self.filtered_stats

    def _pair_models_in_fold(self, models):
        """æ‰¾å‡ºè‡ªç¯æ¨¡å‹å’ŒçœŸå®è¾¹æ¨¡å‹çš„é…å¯¹å…³ç³»"""
        paired_models = {}

        # æŒ‰ç±»åˆ«åˆ†ç»„
        selfloop_models = [m for m in models if m["model_category"] == "gat_selfloop"]
        realedge_models = [m for m in models if m["model_category"] == "gat_realedge"]

        # å¯»æ‰¾é…å¯¹
        for selfloop_model in selfloop_models:
            for realedge_model in realedge_models:
                # æ£€æŸ¥è¶…å‚æ•°æ˜¯å¦åŒ¹é…
                if (
                    selfloop_model["loss_type"] == realedge_model["loss_type"]
                    and selfloop_model["hidden_dim"] == realedge_model["hidden_dim"]
                    and selfloop_model["penalty"] == realedge_model["penalty"]
                    and selfloop_model["alpha"] == realedge_model["alpha"]
                    and selfloop_model["dropout"] == realedge_model["dropout"]
                    and selfloop_model["learning_rate"]
                    == realedge_model["learning_rate"]
                    and selfloop_model["heads"] == realedge_model["heads"]
                ):

                    # å»ºç«‹åŒå‘æ˜ å°„
                    paired_models[selfloop_model["model_name"]] = realedge_model[
                        "model_name"
                    ]
                    paired_models[realedge_model["model_name"]] = selfloop_model[
                        "model_name"
                    ]
                    break

        return paired_models

    def analyze_overall_performance(self):
        """åˆ†ææ•´ä½“æ€§èƒ½å¹¶ç”Ÿæˆæ‘˜è¦"""
        # è·å–æ ‡è®°ä¸ºæœ€ä½³çš„æ¨¡å‹
        best_models = self.model_performance_df[self.model_performance_df["is_best"]]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ€ä½³æ¨¡å‹(å¯èƒ½æ˜¯è¢«è¿‡æ»¤æ‰äº†)ï¼Œå¤„ç†è¿™ç§æƒ…å†µ
        if best_models.empty:
            print(
                "âš  è­¦å‘Š: æ‰€æœ‰æ ‡è®°ä¸º'æœ€ä½³'çš„æ¨¡å‹éƒ½è¢«è¿‡æ»¤æ‰äº†ï¼Œä½¿ç”¨å‰©ä½™æ¨¡å‹ä¸­è¡¨ç°æœ€å¥½çš„ä½œä¸ºæ›¿ä»£"
            )

            # æŒ‰ç…§foldåˆ†ç»„ï¼Œä¸ºæ¯ä¸ªfoldé€‰æ‹©ä¸€ä¸ªæ–°çš„æœ€ä½³æ¨¡å‹
            for fold in self.model_performance_df["fold"].unique():
                fold_models = self.model_performance_df[
                    self.model_performance_df["fold"] == fold
                ]

                # å…ˆå°è¯•ä»éæƒé‡æ¨¡å‹ä¸­é€‰æ‹©
                fold_nn_models = fold_models[
                    fold_models["model_category"] != "baseline_weights"
                ]

                if not fold_nn_models.empty:
                    # æ‰¾å‡ºè®­ç»ƒå¾—åˆ†æœ€é«˜çš„éæƒé‡æ¨¡å‹å¹¶æ ‡è®°ä¸ºæœ€ä½³
                    best_idx = fold_nn_models["train_score"].idxmax()
                    self.model_performance_df.loc[best_idx, "is_best"] = True
                    print(
                        f"  æŠ˜ {fold}: æ ‡è®° {self.model_performance_df.loc[best_idx, 'model_name']} ä¸ºæ–°çš„æœ€ä½³æ¨¡å‹"
                    )
                else:
                    print(f"  æŠ˜ {fold}: æ²¡æœ‰æ‰¾åˆ°éæƒé‡æ¨¡å‹")

            # é‡æ–°è·å–æ›´æ–°åçš„æœ€ä½³æ¨¡å‹åˆ—è¡¨
            best_models = self.model_performance_df[
                self.model_performance_df["is_best"]
            ]

        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary = {
            "æ€»æ¨¡å‹æ•°": len(self.model_performance_df),
            "è¿‡æ»¤æ‰çš„æ¨¡å‹æ•°": self.filtered_stats.get("unconverged_models", 0)
            + self.filtered_stats.get("underperforming_models", 0),
            "æœ€ä½³æ¨¡å‹fold": best_models["fold"].tolist(),
            "æœ€ä½³æ¨¡å‹": best_models["model_name"].tolist(),
            "æœ€ä½³æ¨¡å‹è®­ç»ƒå¾—åˆ†": best_models["train_score"].tolist(),
            "æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†": best_models["test_score"].tolist(),
        }

        # ä¿å­˜æ‘˜è¦
        with open(os.path.join(self.output_dir, "performance_summary.json"), "w") as f:
            json.dump(summary, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)

        # æ‰“å°æ‘˜è¦
        print("\n===== æ€§èƒ½æ‘˜è¦ =====")
        print(f"æ€»æ¨¡å‹æ•°: {summary['æ€»æ¨¡å‹æ•°']}")

        for i, model in enumerate(summary["æœ€ä½³æ¨¡å‹"]):
            print(f"\næŠ˜ {summary['æœ€ä½³æ¨¡å‹fold'][i]}:")
            print(f"  æœ€ä½³æ¨¡å‹: {model}")
            print(f"  è®­ç»ƒå¾—åˆ†: {summary['æœ€ä½³æ¨¡å‹è®­ç»ƒå¾—åˆ†'][i]:.4f}")
            print(f"  æµ‹è¯•å¾—åˆ†: {summary['æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†'][i]:.4f}")

        return summary

    def analyze_parameter_impact(self):
        """åˆ†æä¸åŒå‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“"""
        # è¦åˆ†æçš„å‚æ•°åˆ—è¡¨
        params_to_analyze = [
            "heads",
            "loss_type",
            "hidden_dim",
            "penalty",
            "learning_rate",
            "alpha",
            "dropout",
        ]

        # åˆ›å»ºå‚æ•°å½±å“åˆ†æç›®å½•
        param_dir = os.path.join(self.output_dir, "parameter_impact")
        os.makedirs(param_dir, exist_ok=True)

        # åˆ†ææ¯ä¸ªå‚æ•°
        param_impact = {}

        nn_models_df = self.model_performance_df[
            self.model_performance_df["model_category"] != "baseline_weights"
        ]

        for param in params_to_analyze:
            if param not in nn_models_df.columns:
                print(f"âš  è­¦å‘Šï¼šå‚æ•° {param} ä¸åœ¨æ•°æ®é›†åˆ—ä¸­ï¼Œè·³è¿‡åˆ†æ")
                continue

            unique_values = nn_models_df[param].unique()
            if len(unique_values) <= 1:
                continue

            # è®¡ç®—æ¯ä¸ªå‚æ•°å€¼çš„å¹³å‡æ€§èƒ½
            impact_data = []
            unique_values = nn_models_df[param].unique()

            for value in unique_values:
                if not pd.isna(value):
                    # ç²¾ç¡®åŒ¹é…éNaNå€¼
                    subset = nn_models_df[nn_models_df[param] == value]

                    if not subset.empty:
                        impact_data.append(
                            {
                                "param_value": value,
                                "count": len(subset),
                                "mean_train": subset["train_score"].mean(),
                                "mean_test": subset["test_score"].mean(),
                                "best_count": subset["is_best"].sum(),
                            }
                        )

            # ç„¶åç‰¹æ®Šå¤„ç†NaNå€¼
            nan_subset = nn_models_df[pd.isna(nn_models_df[param])]
            if not nan_subset.empty:
                impact_data.append(
                    {
                        "param_value": np.nan,  # æ˜ç¡®ä½¿ç”¨np.nan
                        "count": len(nan_subset),
                        "mean_train": nan_subset["train_score"].mean(),
                        "mean_test": nan_subset["test_score"].mean(),
                        "best_count": nan_subset["is_best"].sum(),
                    }
                )

            # ä¿å­˜å‚æ•°å½±å“æ•°æ®
            if impact_data:
                param_impact[param] = impact_data

                # åˆ›å»ºå‚æ•°å½±å“å›¾è¡¨
                self.plot_parameter_impact(param, impact_data, param_dir)
            else:
                print(f"âš  è­¦å‘Šï¼šå‚æ•° {param} æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯åˆ†æ")

        # ä¿å­˜å‚æ•°å½±å“æ‘˜è¦
        with open(
            os.path.join(self.output_dir, "parameter_impact_summary.json"), "w"
        ) as f:
            json.dump(param_impact, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)

        return param_impact

    def plot_parameter_impact(
        self, param: str, impact_data: List[Dict], output_dir: str
    ):
        """
        ç»˜åˆ¶å‚æ•°å½±å“å›¾è¡¨

        å‚æ•°:
            param: å‚æ•°åç§°
            impact_data: å½±å“æ•°æ®åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        # è½¬æ¢ä¸ºDataFrame
        impact_df = pd.DataFrame(impact_data)
        impact_df["param_label"] = impact_df["param_value"].astype(str)

        # æ’åº
        if param in ["hidden_dim", "learning_rate", "alpha", "dropout"]:
            impact_df = impact_df.sort_values(by="param_value")

        # ç»˜åˆ¶è¿‡æ‹Ÿåˆå¯¹æ¯”å›¾
        plt.figure(figsize=(12, 6))

        x = np.arange(len(impact_df))
        width = 0.35

        # è®­ç»ƒå’Œæµ‹è¯•åˆ†æ•°
        plt.bar(x - width / 2, impact_df["mean_train"], width, label="è®­ç»ƒå¾—åˆ†")
        plt.bar(x + width / 2, impact_df["mean_test"], width, label="æµ‹è¯•å¾—åˆ†")

        # æ·»åŠ æ ‡ç­¾
        plt.xlabel(param)
        plt.ylabel("å¹³å‡å¾—åˆ†(MAP)")
        plt.title(f"{param}å‚æ•°å¯¹è®­ç»ƒå’Œæµ‹è¯•æ€§èƒ½çš„å¯¹æ¯”")
        plt.xticks(x, impact_df["param_label"])
        plt.legend()
        plt.grid(axis="y", linestyle="--", alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"{param}_train_test_compare.png"))
        plt.close()

    def analyze_self_loops_effect(self):
        """åˆ†æè‡ªç¯æ¨¡å¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç›´æ¥ä½¿ç”¨é¢„å¤„ç†é˜¶æ®µå»ºç«‹çš„é…å¯¹å…³ç³»"""
        # ç¡®ä¿model_categoryå­—æ®µå­˜åœ¨
        if "model_category" not in self.model_performance_df.columns:
            print("âš  æ•°æ®ä¸­æ²¡æœ‰model_categoryä¿¡æ¯ï¼Œæ— æ³•ç»˜åˆ¶åˆ†ç±»æ¨¡å‹å¯¹æ¯”å›¾")
            return None

        # åˆå§‹åŒ–ç»“æœç»“æ„
        self_loop_stats = {}

        # æŒ‰foldåˆ†æ
        for fold in self.model_performance_df["fold"].unique():
            # è·å–å½“å‰foldçš„æ¨¡å‹
            fold_models = self.model_performance_df[
                self.model_performance_df["fold"] == fold
            ]
            fold_self_loop = fold_models[
                fold_models["model_category"] == "gat_selfloop"
            ]
            fold_real_edge = fold_models[
                fold_models["model_category"] == "gat_realedge"
            ]

            # ä½¿ç”¨é¢„å¤„ç†é˜¶æ®µå·²å»ºç«‹çš„é…å¯¹å…³ç³»
            fold_pairs = []

            # æ£€æŸ¥æ˜¯å¦æœ‰é…å¯¹ä¿¡æ¯
            for _, self_loop_model in fold_self_loop.iterrows():
                model_name = self_loop_model["model_name"]

                # è·å–å¯¹åº”çš„çœŸå®è¾¹æ¨¡å‹åç§°
                if model_name in self.paired_models[fold]:
                    real_edge_name = self.paired_models[fold][model_name]

                    # æŸ¥æ‰¾å¯¹åº”çš„çœŸå®è¾¹æ¨¡å‹
                    real_edge_models = fold_real_edge[
                        fold_real_edge["model_name"] == real_edge_name
                    ]

                    if not real_edge_models.empty:
                        real_edge_model = real_edge_models.iloc[0]

                        pair_info = {
                            "self_loop_model": model_name,
                            "real_edge_model": real_edge_name,
                            "self_loop_train": self_loop_model["train_score"],
                            "real_edge_train": real_edge_model["train_score"],
                            "self_loop_test": self_loop_model["test_score"],
                            "real_edge_test": real_edge_model["test_score"],
                            "real_edge_improvement": real_edge_model["test_score"]
                            - self_loop_model["test_score"],
                        }

                        fold_pairs.append(pair_info)

            # å¯¹ç»“æœæŒ‰çœŸå®è¾¹æ¨¡å‹å¯¹æ¯”è‡ªç¯æ¨¡å‹çš„æå‡é‡é™åºæ’åˆ—
            fold_pairs.sort(key=lambda x: x["real_edge_improvement"], reverse=True)

            # ä¿å­˜å½“å‰foldçš„ç»Ÿè®¡ä¿¡æ¯
            self_loop_stats[fold] = fold_pairs

        # ä¿å­˜è‡ªç¯åˆ†æç»“æœ
        with open(os.path.join(self.output_dir, "self_loops_analysis.json"), "w") as f:
            json.dump(
                self_loop_stats, f, indent=4, ensure_ascii=False, cls=NumpyEncoder
            )

        # ç»˜åˆ¶ä¸åŒç±»åˆ«æ¨¡å‹çš„å¯¹æ¯”æ•£ç‚¹å›¾
        self.plot_model_categories_comparison(self_loop_stats)

        return self_loop_stats

    def plot_model_categories_comparison(self, fold_comparisons):
        """
        ç»˜åˆ¶ä¸åŒç±»åˆ«æ¨¡å‹çš„å¯¹æ¯”æ•£ç‚¹å›¾(MLP, GATè‡ªç¯, GATçœŸå®è¾¹)

        å‚æ•°:
            fold_comparisons: æŒ‰foldç»„ç»‡çš„æ¯”è¾ƒæ•°æ®å­—å…¸
        """
        if not fold_comparisons:
            return

        # å‡†å¤‡æ•°æ®
        all_models = self.model_performance_df.copy()

        # åˆ›å»ºæ•£ç‚¹å›¾
        plt.figure(figsize=(12, 10))

        # ä¸ºæ¯ä¸ªfoldåˆ†é…ä¸åŒé¢œè‰²
        folds = all_models["fold"].unique()
        colors = plt.cm.tab10(np.linspace(0, 1, len(folds)))
        fold_color_map = {fold: colors[i] for i, fold in enumerate(folds)}

        legend_elements = []

        # ä¸ºæ¯ç§æ¨¡å‹ç±»åˆ«å®šä¹‰æ ‡è®°
        markers = {
            "baseline_mlp": "^",  # ä¸‰è§’å½¢
            "gat_selfloop": "s",  # æ–¹å½¢
            "gat_realedge": "o",  # åœ†å½¢
        }

        # æ¨¡å‹ç±»åˆ«åç§°æ˜ å°„
        category_names = {
            "baseline_mlp": "MLPæ¨¡å‹",
            "gat_selfloop": "GATè‡ªç¯æ¨¡å‹",
            "gat_realedge": "GATçœŸå®è¾¹æ¨¡å‹",
        }

        # ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹ç‚¹
        for fold in folds:
            fold_models = all_models[all_models["fold"] == fold]

            # æŒ‰æ¨¡å‹ç±»åˆ«ç»˜åˆ¶
            for category, marker in markers.items():
                category_models = fold_models[fold_models["model_category"] == category]

                if not category_models.empty:
                    plt.scatter(
                        category_models["train_score"],
                        category_models["test_score"],
                        marker=marker,
                        s=80,
                        color=fold_color_map[fold],
                        alpha=0.7,
                        label="_nolegend_",
                    )

                    # åªä¸ºæ¯ä¸ªç±»åˆ«çš„ç¬¬ä¸€ä¸ªfoldæ·»åŠ å›¾ä¾‹
                    if fold == folds[0]:
                        legend_elements.append(
                            Line2D(
                                [0],
                                [0],
                                marker=marker,
                                color="k",
                                linestyle="none",
                                markerfacecolor="none",
                                markeredgewidth=1.5,
                                label=f"{category_names[category]}",
                                markersize=10,
                            )
                        )

            legend_elements.append(
                Line2D(
                    [0],
                    [0],
                    marker="o",
                    color="w",
                    label=f"æŠ˜ {fold}",
                    markerfacecolor=fold_color_map[fold],
                    markersize=10,
                )
            )

        # æ ‡è®°æœ€ä½³æ¨¡å‹
        best_models = all_models[all_models["is_best"]]
        legend_elements.append(
            Line2D(
                [0],
                [0],
                marker="o",
                color="w",
                markeredgecolor="red",
                markerfacecolor="none",
                markersize=12,
                label="æœ€ä½³æ¨¡å‹",
                markeredgewidth=2,
            )
        )
        for _, model in best_models.iterrows():
            category = model["model_category"]
            marker = markers[category]
            plt.scatter(
                model["train_score"],
                model["test_score"],
                marker=marker,
                s=120,
                facecolors="none",
                edgecolors="red",
                linewidths=2,
                label="_nolegend_",
            )

        # ç”¨è™šçº¿è¿æ¥é…å¯¹çš„æ¨¡å‹ - ä½¿ç”¨æŒ‰foldç»„ç»‡çš„é…å¯¹æ¨¡å‹
        for fold, fold_data in fold_comparisons.items():
            for pair in fold_data:
                self_loop_model = all_models[
                    (all_models["model_name"] == pair["self_loop_model"])
                    & (all_models["fold"] == fold)
                ]
                real_edge_model = all_models[
                    (all_models["model_name"] == pair["real_edge_model"])
                    & (all_models["fold"] == fold)
                ]

                if not self_loop_model.empty and not real_edge_model.empty:
                    plt.plot(
                        [
                            self_loop_model.iloc[0]["train_score"],
                            real_edge_model.iloc[0]["train_score"],
                        ],
                        [
                            self_loop_model.iloc[0]["test_score"],
                            real_edge_model.iloc[0]["test_score"],
                        ],
                        "k--",  # é»‘è‰²è™šçº¿
                        alpha=0.5,
                        linewidth=0.7,
                    )

        plt.axis("equal")

        # æ·»åŠ è¾…åŠ©å…ƒç´ 
        plt.xlabel("å›å½’é˜¶æ®µå¾—åˆ†", fontsize=12)
        plt.ylabel("æµ‹è¯•é˜¶æ®µå¾—åˆ†", fontsize=12)
        plt.title("ä¸åŒç±»åˆ«æ¨¡å‹æ€§èƒ½å¯¹æ¯”", fontsize=14)

        plt.legend(handles=legend_elements, loc="best")

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "model_categories_comparison.png"))
        plt.close()

    def analyze_training_time(self):
        """åˆ†æè®­ç»ƒæ—¶é—´"""
        if not self.training_time:
            eprint("âš  æ²¡æœ‰è®­ç»ƒæ—¶é—´æ•°æ®ï¼Œè·³è¿‡åˆ†æ")
            return None

        # è®¡ç®—å¹³å‡æ¯bugå’Œæ¯æ–‡ä»¶çš„è®­ç»ƒæ—¶é—´
        time_stats = {}

        for fold, data in self.training_time.items():
            time_stats[fold] = {
                "total_time_seconds": data["time"],
                "total_time_minutes": data["time"] / 60,
                "total_time_hours": data["time"] / 3600,
                "bugs_count": data["bugs"],
                "files_count": data["files"],
                "seconds_per_bug": (
                    data["time"] / data["bugs"] if data["bugs"] > 0 else 0
                ),
                "seconds_per_file": (
                    data["time"] / data["files"] if data["files"] > 0 else 0
                ),
            }

        # ä¿å­˜ç»“æœ
        with open(
            os.path.join(self.output_dir, "training_time_analysis.json"), "w"
        ) as f:
            json.dump(time_stats, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)

        # æ‰“å°æ—¶é—´ç»Ÿè®¡
        print("\n===== è®­ç»ƒæ—¶é—´åˆ†æ =====")
        for fold, stats in time_stats.items():
            print(f"æŠ˜ {fold}:")
            print(
                f"  æ€»è®­ç»ƒæ—¶é—´: {stats['total_time_hours']:.2f}å°æ—¶ ({stats['total_time_minutes']:.2f}åˆ†é’Ÿ)"
            )
            print(f"  Bugæ•°é‡: {stats['bugs_count']}")
            print(f"  æ–‡ä»¶æ•°é‡: {stats['files_count']}")
            print(f"  å¹³å‡æ¯bugè®­ç»ƒæ—¶é—´: {stats['seconds_per_bug']:.2f}ç§’")
            print(f"  å¹³å‡æ¯æ–‡ä»¶è®­ç»ƒæ—¶é—´: {stats['seconds_per_file']:.2f}ç§’")

        return time_stats

    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š"""

        # è·å–æœ€ä½³æ¨¡å‹è¯¦æƒ…
        best_models = self.model_performance_df[self.model_performance_df["is_best"]]
        best_model_category = best_models["model_category"].iloc[0]

        # æŒ‰æ¨¡å‹ç±»åˆ«åˆ†ç»„è®¡ç®—æœ€ä½³æ€§èƒ½
        fold_category_performance = (
            self.model_performance_df.groupby(["fold", "model_category"])["test_score"]
            .max()
            .reset_index()
        )

        # è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­ä¸€ä¸ªæ¨¡å‹ç±»åˆ«æ˜¯å¦åœ¨æ‰€æœ‰foldä¸­éƒ½ä¼˜äºå¦ä¸€ä¸ªç±»åˆ«
        def is_better_in_all_folds(category1, category2):
            """åˆ¤æ–­category1æ˜¯å¦åœ¨æ‰€æœ‰foldä¸­éƒ½ä¼˜äºcategory2"""
            for fold in fold_category_performance["fold"].unique():
                # è·å–å½“å‰foldä¸­ä¸¤ç§ç±»åˆ«çš„æœ€é«˜å¾—åˆ†
                cat1_score = fold_category_performance[
                    (fold_category_performance["fold"] == fold)
                    & (fold_category_performance["model_category"] == category1)
                ]["test_score"]

                cat2_score = fold_category_performance[
                    (fold_category_performance["fold"] == fold)
                    & (fold_category_performance["model_category"] == category2)
                ]["test_score"]

                # å¦‚æœä»»ä¸€ç±»åˆ«åœ¨è¯¥foldä¸­æ²¡æœ‰æ•°æ®ï¼Œæˆ–cat1ä¸ä¼˜äºcat2ï¼Œåˆ™è¿”å›False
                if (
                    cat1_score.empty
                    or cat2_score.empty
                    or cat1_score.iloc[0] <= cat2_score.iloc[0]
                ):
                    return False

            return True

        # è¾…åŠ©å‡½æ•°ï¼šæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ç±»åˆ«å¹¶è¿”å›ç»“è®º
        def compare_categories(category, name):
            """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ç±»åˆ«ï¼Œè¿”å›æ¯”è¾ƒç»“è®º"""
            if category not in fold_category_performance["model_category"].values:
                return f"æœªæ¯”è¾ƒGATæ¨¡å‹ä¸{name}"

            if is_better_in_all_folds("gat_realedge", category):
                return f"GATæ¨¡å‹è¡¨ç°ä¼˜äº{name}"
            elif is_better_in_all_folds(category, "gat_realedge"):
                return f"{name}è¡¨ç°ä¼˜äºGATæ¨¡å‹"
            else:
                return f"GATæ¨¡å‹ä¸{name}è¡¨ç°æ— æ³•ç¡®å®šæ˜æ˜¾ä¼˜åŠ£"

        # æ‰§è¡Œå„ç§æ¯”è¾ƒ
        findings = [
            compare_categories("baseline_weights", "æƒé‡æ³•"),
            compare_categories("gat_selfloop", "ä»…è‡ªç¯GATæ¨¡å‹"),
            compare_categories("baseline_mlp", "MLPæ¨¡å‹"),
        ]

        # æ•´ä½“æ€§èƒ½ç»Ÿè®¡
        category_performance = (
            self.model_performance_df.groupby("model_category")["test_score"]
            .agg(["max", "mean"])
            .reset_index()
        )

        # å‡†å¤‡æŠ¥å‘Šå†…å®¹
        report = {
            "æ€»ç»“": {
                "è¯„ä¼°æ—¶é—´": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
                "è¯„ä¼°ç›®å½•": self.log_dir,
                "æ¨¡å‹æ€»æ•°": len(self.model_performance_df),
                "æœ€ä½³æ¨¡å‹": best_models["model_name"].tolist(),
                "æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†": best_models["test_score"].tolist(),
                "æœ€ä½³æ¨¡å‹ç±»å‹": {
                    "baseline_mlp": "åŸºçº¿MLPæ¨¡å‹",
                    "baseline_weights": "æƒé‡çº¿æ€§ç»„åˆæ¨¡å‹",
                    "gat_realedge": "GATæ¨¡å‹",
                    "gat_selfloop": "ä»…è‡ªç¯GATæ¨¡å‹",
                }.get(best_model_category, "æœªçŸ¥ç±»å‹"),
            },
            "æ¨¡å‹ç±»åˆ«æ€§èƒ½": category_performance.to_dict(orient="records"),
            "ä¸»è¦å‘ç°": findings,
        }

        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        with open(os.path.join(self.output_dir, "final_report.json"), "w") as f:
            json.dump(report, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)

        # åˆ›å»ºå¯è¯»æ€§æ›´å¥½çš„æ–‡æœ¬æŠ¥å‘Š
        report_text = f"""
=========================================
è‡ªé€‚åº”GATè®­ç»ƒè¯„ä¼°æŠ¥å‘Š
=========================================
è¯„ä¼°æ—¶é—´: {report['æ€»ç»“']['è¯„ä¼°æ—¶é—´']}
è¯„ä¼°ç›®å½•: {report['æ€»ç»“']['è¯„ä¼°ç›®å½•']}
æ¨¡å‹æ€»æ•°: {report['æ€»ç»“']['æ¨¡å‹æ€»æ•°']}

æœ€ä½³æ¨¡å‹: {report['æ€»ç»“']['æœ€ä½³æ¨¡å‹'][0]}
æœ€ä½³æ¨¡å‹ç±»å‹: {report['æ€»ç»“']['æœ€ä½³æ¨¡å‹ç±»å‹']}
æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†: {report['æ€»ç»“']['æœ€ä½³æ¨¡å‹æµ‹è¯•å¾—åˆ†'][0]:.4f}

ä¸»è¦å‘ç°:
{chr(10).join('- ' + finding for finding in report['ä¸»è¦å‘ç°'])}
=========================================
        """

        with open(
            os.path.join(self.output_dir, "final_report.txt"), "w", encoding="utf-8"
        ) as f:
            f.write(report_text)

        print("\n" + report_text)

        return report

    def run_all_analyses(self):
        """è¿è¡Œæ‰€æœ‰åˆ†æ"""
        # åŠ è½½æ—¥å¿—æ•°æ®
        self.load_logs()

        # é¢„å¤„ç†æ•°æ®
        self.preprocess_data()

        # è¿è¡Œå„ç§åˆ†æ
        self.analyze_overall_performance()
        self.analyze_parameter_impact()
        self.analyze_self_loops_effect()
        self.analyze_training_time()

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report()

        print(f"\nâœ“ å…¨éƒ¨åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.output_dir}")


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="åˆ†æè‡ªé€‚åº”GATè®­ç»ƒæ—¥å¿—å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
    parser.add_argument("log_dir", help="åŒ…å«è®­ç»ƒæ—¥å¿—æ–‡ä»¶çš„ç›®å½•è·¯å¾„")
    parser.add_argument(
        "--output", "-o", help="è¾“å‡ºç›®å½• (é»˜è®¤ä¸ºlog_dir/analysis_results)"
    )

    args = parser.parse_args()

    # args = argparse.Namespace(
    #     log_dir="tomcat_auto_20250320041613/",
    #     output=None,
    # )

    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œåˆ†æ
    evaluator = AdaptiveGATEvaluator(args.log_dir)

    if args.output:
        evaluator.output_dir = args.output
        os.makedirs(evaluator.output_dir, exist_ok=True)

    evaluator.run_all_analyses()


class NumpyEncoder(json.JSONEncoder):
    """å¤„ç†NumPyç±»å‹çš„JSONç¼–ç å™¨"""

    def default(self, obj):
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):  # æ·»åŠ å¯¹NumPyå¸ƒå°”ç±»å‹çš„å¤„ç†
            return bool(obj)
        return super(NumpyEncoder, self).default(obj)


if __name__ == "__main__":
    main()
